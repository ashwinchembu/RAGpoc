{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure RAG POC - Document Search with LLM\n",
        "\n",
        "This notebook demonstrates how to implement Retrieval Augmented Generation (RAG) using Azure services for efficient document search and Q&A.\n",
        "\n",
        "## Architecture:\n",
        "1. **Document Processing**: Load and chunk documents\n",
        "2. **Embeddings**: Generate embeddings using Azure OpenAI\n",
        "3. **Vector Store**: Store embeddings for similarity search (using FAISS for POC)\n",
        "4. **RAG Pipeline**: Retrieve relevant docs and generate answers using Azure OpenAI\n",
        "\n",
        "## Prerequisites:\n",
        "- Azure OpenAI Service access\n",
        "- API key and endpoint\n",
        "- Deployed models for embeddings and completions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (1.12.0)\n",
            "Collecting langchain==0.1.6\n",
            "  Using cached langchain-0.1.6-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-openai==0.0.5\n",
            "  Using cached langchain_openai-0.0.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: faiss-cpu==1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (1.7.4)\n",
            "Collecting tiktoken==0.5.2\n",
            "  Using cached tiktoken-0.5.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: python-dotenv==1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
            "Requirement already satisfied: pypdf==4.0.1 in /opt/anaconda3/lib/python3.11/site-packages (4.0.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai==1.12.0) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai==1.12.0) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai==1.12.0) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai==1.12.0) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai==1.12.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai==1.12.0) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/lib/python3.11/site-packages (from openai==1.12.0) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain==0.1.6) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.11/site-packages (from langchain==0.1.6) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain==0.1.6) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.11/site-packages (from langchain==0.1.6) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.11/site-packages (from langchain==0.1.6) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.18 (from langchain==0.1.6)\n",
            "  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain-core<0.2,>=0.1.22 (from langchain==0.1.6)\n",
            "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain==0.1.6)\n",
            "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /opt/anaconda3/lib/python3.11/site-packages (from langchain==0.1.6) (1.26.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain==0.1.6) (8.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken==0.5.2) (2024.7.24)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.9.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.12.0) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.12.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.6) (2.1)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community<0.1,>=0.0.18 (from langchain==0.1.6)\n",
            "  Using cached langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Using cached langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Using cached langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Using cached langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Using cached langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Using cached langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Using cached langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
            "  Using cached langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-core<0.2,>=0.1.22 (from langchain==0.1.6)\n",
            "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain==0.1.6)\n",
            "  Using cached langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.22->langchain==0.1.6)\n",
            "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.12.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.12.0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.12.0) (0.4.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (1.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
            "Using cached langchain-0.1.6-py3-none-any.whl (811 kB)\n",
            "Using cached langchain_openai-0.0.5-py3-none-any.whl (29 kB)\n",
            "Using cached tiktoken-0.5.2-cp311-cp311-macosx_11_0_arm64.whl (953 kB)\n",
            "Using cached langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
            "Using cached langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "Using cached langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
            "Installing collected packages: packaging, tiktoken, langsmith, langchain-core, langchain-openai, langchain-community, langchain\n",
            "\u001b[2K  Attempting uninstall: packaging\n",
            "\u001b[2K    Found existing installation: packaging None\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-no-record-file\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Cannot uninstall packaging None\n",
            "\u001b[31mâ•°â”€>\u001b[0m The package's contents are unknown: no RECORD file was found for packaging.\n",
            "\n",
            "\u001b[1;36mhint\u001b[0m: You might be able to recover from this via: \u001b[32mpip install --force-reinstall --no-deps packaging==24.2\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/7\u001b[0m [packaging]\n",
            "\u001b[1A\u001b[2KNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install openai==1.12.0 langchain==0.1.6 langchain-openai==0.0.5 \\\n",
        "    faiss-cpu==1.7.4 tiktoken==0.5.2 python-dotenv==1.0.0 \\\n",
        "    pypdf==4.0.1 requests beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Configuration loaded\n",
            "  Endpoint: https://ashwin-demo.openai.azure.com/\n",
            "  Embedding Model: text-embedding-ada-002\n",
            "  Chat Model: gpt-4.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Azure OpenAI Configuration\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://your-resource.openai.azure.com/\")\n",
        "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", \"your-api-key-here\")\n",
        "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
        "\n",
        "# Model deployment names (these should match your Azure OpenAI deployments)\n",
        "EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\", \"text-embedding-ada-002\")\n",
        "CHAT_DEPLOYMENT = os.getenv(\"AZURE_CHAT_DEPLOYMENT\", \"gpt-4\")\n",
        "\n",
        "print(\"âœ“ Configuration loaded\")\n",
        "print(f\"  Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
        "print(f\"  Embedding Model: {EMBEDDING_DEPLOYMENT}\")\n",
        "print(f\"  Chat Model: {CHAT_DEPLOYMENT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Azure OpenAI Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Azure OpenAI client initialized\n"
          ]
        }
      ],
      "source": [
        "from openai import AzureOpenAI\n",
        "\n",
        "# Initialize Azure OpenAI client\n",
        "client = AzureOpenAI(\n",
        "    api_key=AZURE_OPENAI_API_KEY,\n",
        "    api_version=AZURE_OPENAI_API_VERSION,\n",
        "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
        ")\n",
        "\n",
        "print(\"âœ“ Azure OpenAI client initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Document Loading and Processing\n",
        "\n",
        "For this POC, we'll use sample publicly available documents. You can replace this with your own 30 documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Loaded 43 REAL documents from public API sources\n",
            "\n",
            "Document sources breakdown:\n",
            "  - Wikipedia: 18 documents\n",
            "  - REST Countries API: 10 documents\n",
            "  - Open Library: 15 documents\n",
            "\n",
            "Sample document titles:\n",
            "  1. E-commerce\n",
            "  2. Customer service\n",
            "  3. Online shopping\n",
            "  4. Warranty\n",
            "  5. Payment system\n",
            "  6. Customer relationship management\n",
            "  7. Consumer protection\n",
            "  8. Supply chain\n",
            "  9. Inventory\n",
            "  10. Retail\n",
            "  ... and 33 more\n",
            "\n",
            "ðŸŒ These are REAL documents fetched from:\n",
            "   â€¢ Wikipedia (E-commerce topics)\n",
            "   â€¢ REST Countries API (Country profiles)\n",
            "   â€¢ Open Library (Business books)\n"
          ]
        }
      ],
      "source": [
        "# Load real documents from public sources (fetched from APIs)\n",
        "import json\n",
        "\n",
        "# Load documents fetched from Wikipedia, REST Countries, and Open Library APIs\n",
        "with open('fetched_documents.json', 'r', encoding='utf-8') as f:\n",
        "    sample_documents = json.load(f)\n",
        "\n",
        "# Analyze the documents\n",
        "sources = {}\n",
        "for doc in sample_documents:\n",
        "    source = doc.get('source', 'Unknown')\n",
        "    sources[source] = sources.get(source, 0) + 1\n",
        "\n",
        "print(f\"âœ“ Loaded {len(sample_documents)} REAL documents from public API sources\")\n",
        "print(\"\\nDocument sources breakdown:\")\n",
        "for source, count in sources.items():\n",
        "    print(f\"  - {source}: {count} documents\")\n",
        "\n",
        "print(\"\\nSample document titles:\")\n",
        "for i, doc in enumerate(sample_documents[:10], 1):\n",
        "    title = doc['title'][:60] + \"...\" if len(doc['title']) > 60 else doc['title']\n",
        "    print(f\"  {i}. {title}\")\n",
        "if len(sample_documents) > 10:\n",
        "    print(f\"  ... and {len(sample_documents) - 10} more\")\n",
        "\n",
        "print(f\"\\nðŸŒ These are REAL documents fetched from:\")\n",
        "print(\"   â€¢ Wikipedia (E-commerce topics)\")\n",
        "print(\"   â€¢ REST Countries API (Country profiles)\")\n",
        "print(\"   â€¢ Open Library (Business books)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Text Chunking\n",
        "\n",
        "Split documents into smaller chunks for better retrieval performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Created 202 chunks from 43 documents\n",
            "\n",
            "Example chunk:\n",
            "  Title: E-commerce\n",
            "  Text: E-commerce (electronic commerce) refers to commercial activities including the electronic buying or ...\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def chunk_documents(documents, chunk_size=500, chunk_overlap=50):\n",
        "    \"\"\"\n",
        "    Split documents into smaller chunks for embedding.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    chunks = []\n",
        "    for doc in documents:\n",
        "        splits = text_splitter.split_text(doc['content'])\n",
        "        for split in splits:\n",
        "            chunks.append({\n",
        "                'text': split,\n",
        "                'title': doc['title'],\n",
        "                'source': doc['title']\n",
        "            })\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Chunk the documents\n",
        "document_chunks = chunk_documents(sample_documents)\n",
        "\n",
        "print(f\"âœ“ Created {len(document_chunks)} chunks from {len(sample_documents)} documents\")\n",
        "print(f\"\\nExample chunk:\")\n",
        "print(f\"  Title: {document_chunks[0]['title']}\")\n",
        "print(f\"  Text: {document_chunks[0]['text'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Embeddings using Azure OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings... This may take a few minutes.\n",
            "âœ“ Generated 202 embeddings\n",
            "  Embedding dimension: 1536\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def get_embeddings(texts: List[str], batch_size=16) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts using Azure OpenAI.\n",
        "    Processes in batches to handle rate limits.\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        \n",
        "        try:\n",
        "            response = client.embeddings.create(\n",
        "                input=batch,\n",
        "                model=EMBEDDING_DEPLOYMENT\n",
        "            )\n",
        "            \n",
        "            batch_embeddings = [item.embedding for item in response.data]\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "            \n",
        "            # Small delay to avoid rate limiting\n",
        "            if i + batch_size < len(texts):\n",
        "                time.sleep(0.5)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error generating embeddings for batch {i}: {e}\")\n",
        "            raise\n",
        "    \n",
        "    return all_embeddings\n",
        "\n",
        "# Generate embeddings for all chunks\n",
        "print(\"Generating embeddings... This may take a few minutes.\")\n",
        "chunk_texts = [chunk['text'] for chunk in document_chunks]\n",
        "embeddings = get_embeddings(chunk_texts)\n",
        "\n",
        "print(f\"âœ“ Generated {len(embeddings)} embeddings\")\n",
        "print(f\"  Embedding dimension: {len(embeddings[0])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Vector Store with FAISS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Vector store initialized with 202 vectors\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "\n",
        "class VectorStore:\n",
        "    def __init__(self, embeddings, documents):\n",
        "        \"\"\"\n",
        "        Initialize FAISS vector store for similarity search.\n",
        "        \"\"\"\n",
        "        self.documents = documents\n",
        "        self.embeddings = np.array(embeddings).astype('float32')\n",
        "        \n",
        "        # Create FAISS index\n",
        "        dimension = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index.add(self.embeddings)\n",
        "        \n",
        "        print(f\"âœ“ Vector store initialized with {self.index.ntotal} vectors\")\n",
        "    \n",
        "    def search(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Search for k most similar documents to the query.\n",
        "        \"\"\"\n",
        "        query_vector = np.array([query_embedding]).astype('float32')\n",
        "        distances, indices = self.index.search(query_vector, k)\n",
        "        \n",
        "        results = []\n",
        "        for idx, distance in zip(indices[0], distances[0]):\n",
        "            results.append({\n",
        "                'document': self.documents[idx],\n",
        "                'distance': float(distance),\n",
        "                'similarity': 1 / (1 + float(distance))  # Convert distance to similarity\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Create vector store\n",
        "vector_store = VectorStore(embeddings, document_chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. RAG Pipeline Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ RAG pipeline ready\n"
          ]
        }
      ],
      "source": [
        "def rag_query(question: str, top_k: int = 3, temperature: float = 0.3) -> dict:\n",
        "    question_embedding = get_embeddings([question])[0]\n",
        "    retrieved_docs = vector_store.search(question_embedding, k=top_k)\n",
        "    \n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"{doc['document']['title']}: {doc['document']['text']}\"\n",
        "        for doc in retrieved_docs\n",
        "    ])\n",
        "    \n",
        "    system_prompt = \"Answer based on the provided documents. If information isn't in the documents, say so.\"\n",
        "    \n",
        "    user_prompt = f\"Documents:\\n{context}\\n\\nQuestion: {question}\"\n",
        "    \n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=CHAT_DEPLOYMENT,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=300\n",
        "        )\n",
        "        answer = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        answer = f\"Error: {e}\"\n",
        "    \n",
        "    return {\n",
        "        'question': question,\n",
        "        'answer': answer,\n",
        "        'sources': [doc['document']['title'] for doc in retrieved_docs],\n",
        "        'retrieved_docs': retrieved_docs\n",
        "    }\n",
        "\n",
        "print(\"âœ“ RAG ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_answer(result):\n",
        "    print(f\"\\nQ: {result['question']}\")\n",
        "    print(f\"\\nA: {result['answer']}\")\n",
        "    print(f\"\\nSources: {', '.join(result['sources'])}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "QUESTION: What is your return policy and how long do I have to return an item?\n",
            "================================================================================\n",
            "\n",
            "ANSWER:\n",
            "I don't have information about a specific retailer's return policy or the exact time frame for returns in my knowledge base. Return policies can vary depending on the retailer, and they usually require a receipt and that the merchandise is in resellable condition. Some retailers may offer exchanges or store credit if you don't have a receipt or if the initial refund period has passed. Please check with the specific retailer for their detailed return policy and allowed return period. (Document: Product return)\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "SOURCES USED (5 documents):\n",
            "  1. Product return\n",
            "  2. Product return\n",
            "  3. Product return\n",
            "  4. Product return\n",
            "  5. Product return\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "RETRIEVED DOCUMENT SNIPPETS:\n",
            "\n",
            "  [1] Product return (Similarity: 0.754)\n",
            "      Many retailers will accept returns provided that the customer has a receipt as a proof of purchase, and that certain other conditions, which depend on...\n",
            "\n",
            "  [2] Product return (Similarity: 0.746)\n",
            "      .  In some cases, only exchanges or store credit are offered, again usually only without a receipt, or after an initial refund period has passed. Some...\n",
            "\n",
            "  [3] Product return (Similarity: 0.736)\n",
            "      In retail, a product return is the process of a customer taking previously purchased merchandise back to the retailer, and in turn receiving a refund ...\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with real questions\n",
        "result = rag_query(\"What's e-commerce?\")\n",
        "display_answer(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "QUESTION: How much does shipping cost and when will I receive my order?\n",
            "================================================================================\n",
            "\n",
            "ANSWER:\n",
            "I don't have information about shipping costs or delivery times in my knowledge base.\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "SOURCES USED (5 documents):\n",
            "  1. Shopping cart software\n",
            "  2. Customer service\n",
            "  3. Product return\n",
            "  4. Online shopping\n",
            "  5. Product return\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "RETRIEVED DOCUMENT SNIPPETS:\n",
            "\n",
            "  [1] Shopping cart software (Similarity: 0.710)\n",
            "      The software allows online shopping customers to accumulate a list of items for purchase. At the point of sale, the software typically calculates a to...\n",
            "\n",
            "  [2] Customer service (Similarity: 0.695)\n",
            "      Customer support...\n",
            "\n",
            "  [3] Product return (Similarity: 0.689)\n",
            "      .  In some cases, only exchanges or store credit are offered, again usually only without a receipt, or after an initial refund period has passed. Some...\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result = rag_query(\"How many people live in Japan?\")\n",
        "display_answer(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "QUESTION: What payment methods do you accept?\n",
            "================================================================================\n",
            "\n",
            "ANSWER:\n",
            "I don't have information about that in my knowledge base.\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "SOURCES USED (5 documents):\n",
            "  1. Payment system\n",
            "  2. Payment system\n",
            "  3. Payment gateway\n",
            "  4. Online shopping\n",
            "  5. Payment gateway\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "RETRIEVED DOCUMENT SNIPPETS:\n",
            "\n",
            "  [1] Payment system (Similarity: 0.738)\n",
            "      Modern payment systems use cash-substitutes as compared to traditional payment systems. This includes debit cards, credit cards, electronic funds tran...\n",
            "\n",
            "  [2] Payment system (Similarity: 0.727)\n",
            "      . Some payment systems also include credit mechanisms, which are essentially a different aspect of payment....\n",
            "\n",
            "  [3] Payment gateway (Similarity: 0.723)\n",
            "      A payment gateway facilitates a payment transaction by the transfer of information between a payment portal (such as a website, mobile phone or intera...\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result = rag_query(\"What currency does Brazil use?\")\n",
        "display_answer(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Custom Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try your own questions\n",
        "question = \"How do payment systems work?\"\n",
        "result = rag_query(question)\n",
        "display_answer(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Load Your Own Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Document loading function ready\n"
          ]
        }
      ],
      "source": [
        "# Load your own documents\n",
        "def load_custom_docs(file_paths):\n",
        "    docs = []\n",
        "    for path in file_paths:\n",
        "        with open(path, 'r') as f:\n",
        "            content = f.read()\n",
        "        docs.append({'title': os.path.basename(path), 'content': content})\n",
        "    return docs\n",
        "\n",
        "# Example: Load from a folder\n",
        "# my_docs = load_custom_docs(['path/to/doc1.txt', 'path/to/doc2.txt'])\n",
        "# chunks = chunk_documents(my_docs)\n",
        "# embeddings = get_embeddings([c['text'] for c in chunks])\n",
        "# vector_store = VectorStore(embeddings, chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
